# Blog: Prediction Challenges {#predblogs}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy=TRUE)
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```


Each prediction challenge is based on the training data generated synthetically with some embedded patterns. Students can either build their own prediction models from scratch (coding their own prediction models without using R libraries, “free style”) or utilize R libraries for a multitude of machine learning methods. These methods range from decision trees (rpart, recursive partitioning) through linear regression (lm), svm and even neural networks. Students  test their prediction models on the testing data. We use Kaggle to automatically calculate the prediction errors and rank student solutions by prediction accuracy. Depending on the type of independent, target variable we use either prediction accuracy or MSE. 

As data creators we know the method of data generation, therefore we can construct the *perfect* prediction model, Accuracy of the perfect prediction model provides a soft upper bound for prediction accuracy of all possible prediction models, created without knowing how data was generated. Thus, there is no general, absolute, notion of a “good” prediction model. It all depends on the data. Sometimes prediction accuracy of 60% is excellent. For other data sets, accuracy of 95% may not be good enough. 

If the accuracy of a prediction model is near the accuracy of a perfect prediction model, such a model is definitely good. Turns out that, to our initial surprise, for some data sets, students created prediction models with better accuracy than corresponding perfect prediction models. We will discuss it below.


---

## Prediction Challange 1.

For this prediction challenge we used our favorite dataset, the Moody dataset, and predicted the Grade category of all students. The Grade category had only 2 factors: *Pass* OR *Fail*.

Let look at a snippet of the moody dataset used for training in this challenge.

```{r,echo=FALSE}
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/M2021train.csv") #web load
temp<-knitr::kable(
  head(realestate, 10), caption = 'Snippet of Moody Dataset(TRAINING) for Prediction Challenge 1',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")

```


<!-- Lets look at the snippet of the moody dataset for testing. -->

<!-- ```{r,echo=FALSE} -->
<!-- realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/M2021test-students.csv") #web load -->
<!-- temp<-knitr::kable( -->
<!--   head(realestate, 10), caption = 'Snippet of Moody Dataset(TESTING) for Prediction Challenge 1', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- library(kableExtra) -->
<!-- kableExtra::scroll_box(temp,width = "100%") -->
<!-- ``` -->

<!-- We can see that the *Grade* attribute is not present in this dataset, since it is the attribute that will be predicted using our analysis of the training dataset. -->

<!-- Also, lets look at the submission file for prediction challenge 1. -->

<!-- ```{r,echo=FALSE} -->
<!-- realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/M2021test-submission-file.csv") #web load -->
<!-- temp<-knitr::kable( -->
<!--   head(realestate, 10), caption = 'Snippet of Submission file for Prediction Challenge 1', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- library(kableExtra) -->
<!-- kableExtra::scroll_box(temp,width = "100%") -->
<!-- ``` -->

<!-- We can see there are only 2 columns *Studentid* and *Grade*. *Studentid* column's entries corresponds/are similar to the *Studentid* column of the testing data. Thus we need to just fill the *Grade* column with appropriate grade predicted by our analysis, corresponding to the same *Studentid* values in both test and submission data. -->

<!-- Now that we have seen the data, feel free to go to the Kaggle site of this prediction challenge and take the challenge yourself. The link for challenge: [Prediction Challenge 1](https://www.kaggle.com/t/8099c3c8bd5940928d102a6ddda0ee3d){target="_blank"} -->

---

### How the data was generated for Challenge 1

Professor Moody data set has been synthetically generated using random generator which follows probabilistic rules implementing "secret patterns" which we embedded in the data. 

These patterns are presented below in the form of decision tree. For example a rule that statistics major with score over 60, pass the class - reflects the generated data in which high percentage (but not 100%) of such students indeed pass Moody's class. There will always be random exceptions to these rules.  But majority of stat students with score above 60 will pass the class

The data is based on a tree given by the following conditions:

``` text
Tree which is embedded in the data (secret pattern for Moody -challenge1/2)

Major
   Stat
      Score > 60 Pass
      Score <= 60  Fail
   Comm
      Score >40  Pass
      Score <=40
         Texting = Rarely   Fail
          Texting = Always  PAss
   Polsci
      Score >50  Pass
      Score <=50  
         Questions = rare    Fail   
         Questions = always  Pass
   Cs
      Score >70      Pass
      Score <=70
        Seniority= Freshman 
             Score >50 Pass
             Score <=50 
               Attendance >=60
                    Score > 40   Pass
                    Score <=40   Fail
               Attendance <60    Fail  
         Seniority= Sophomore
               Score >50         Pass
               Score <=50        Fail
         Seniority= Junior       Fail
         Seniority = Senior Fail 
```

We can see this in pictorial representation below based on each subset of Majors.

- For Stats Major:
  - We can see that the rule was very simple, with the final grade decided based on only the *Score* attribute of the students.
  - ![The tree used for predicting Stats Major students grade](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/predblog/pred1stat.svg)
  - Thus finding this pattern would have been easier for students

- For Communication Major Students:
  - The grade prediction for students from the Communication Major was based not only on the *Score* attribute but was also based on *Texting* attribute of the students records.
  - ![The tree used for predicting Communications Major students grade](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/predblog/pred1comm.svg)
  - As we can see, finding this pattern would have been not that difficult.

- For Political Science Major Students:
  - The grade prediction for students from the Political Science Major was based not only on the *Score* attribute but was also based on *Questions* attribute of the students records.
  - ![The tree used for predicting Political Science Major students grade](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/predblog/pred1polsc.svg)
  - As we can see, finding this pattern would have been not that difficult.
  
- For Computer Science Major Students:
  - The grade prediction for students from the Computer Science Major was the most involved and was based on various students attribute.
  - Attributes like *Score, Seniority and Attendance* were involved in prediction, and the subsetting conditions were very complex. 
  - ![The tree used for predicting Communications Major students grade](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/predblog/pred1cs.svg)
  - As we can see, finding this huge pattern would have been very difficult for students.
  
If you want to see a well detailed data analysis of the dataset based on Majors as subset, then please look at *Rohit Manjunath's* submission in the Top Submission section for prediction challenge 1.
  
- **How the data was generated using R**
  - You can see a simple way to write the data using the above patterns.
  
```{r,height=700}
# Load Data
dat<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/M2021test-students.csv",stringsAsFactors = T)

# Create an all Fail Category/Predicted Value Vector and append it to the testing dataset.
dat$Grade<-rep('Fail', nrow(dat))

# The various conditions used to predict grade attribute.

# For Major = Stat
dat[dat$Major=='Stat' & dat$Score>60,]$Grade <-'Pass'

# For Major = Comm
dat[dat$Major=='Communication' & dat$Score>40,]$Grade <-'Pass'
dat[dat$Major=='Communication' & dat$Score<=40 & dat$Texting=='Always',]$Grade <-'Pass'

# For Major = Polsci
dat[dat$Major=='Polsci' & dat$Score>50,]$Grade <-'Pass'
dat[dat$Major=='Polsci' & dat$Score<=50 & dat$Questions=='Always',]$Grade <-'Pass'

# For Major = Cs
dat[dat$Major=='Cs' & dat$Score>70,]$Grade <-'Pass'
dat[dat$Major=='Cs' & dat$Score<=70 & dat$Seniority=='Freshman' & dat$Score>50,]$Grade <-'Pass'
dat[dat$Major=='Cs' & dat$Score<=70 & dat$Seniority=='Freshman' & dat$Attendance >=60 & dat$Score>40,]$Grade <-'Pass'
dat[dat$Major=='Cs' & dat$Score<=70 & dat$Seniority=='Sophomore' & dat$Score>50,]$Grade <-'Pass'


# Compare it with the ideal predictions for checking accuracy of our predictions.
answers<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/M2021test_answer.csv",stringsAsFactors = T)
mean(dat$Grade==answers$Grade) # Accuracy
```

Accuracy of the perfect prediction model defined above was around 83% on the testing data set. 

In the next section we review some top student submissions. Surprisingly all top 10 student solutions achieve accuracy beating the ideal 83% accuracy by a solid few percentage points. In other words, these students do better than the prediction model which is fully aware of the rules used in data generation!  How is this possible?

There may be several reasons for this. First,the synthetically  generated data satisfies some spurious, random patterns, in addition to the patterns built into the data generation process. The smaller the generated data set, the more likely these spurious patterns are.  Perfect prediction model will not incorporate these spurious patterns. Prediction models built by students on the basis of the training data set may take advantage of these spurious patterns. Summarizing, the actual data set has some additional noise, and some random patterns which are generated as unintended side effects of our generation procedure. The larger our generated data sets are, the less prominent these spurious patterns are.  Typically our data sets have a few thousand tuples. This is not large enough to combat the law of small numbers - some extreme patterns appearing randomly. 

Another reason could be getting away with overfitting - since even though we have used testing data which is different from training data, we only tested the student prediction models once, against one specific test data set. The students' models may have overfit the training data and testing data as well (since only one testing data set was used). If we tested against multiple testing data sets, this overfitting effect may have vanished. 


---

### Top Submissions for Challenge 1.

Students with accuracy over 60% were considered passed for this prediction challenge.

1. *Jeremy Prasad*     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred11">Jeremy's PPT</button>
<div id="pred11" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/jeremypred1.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - Jeremy performed exceptionally well in this prediction challenge.
  - His approach was a iterative learning process, where at each step after performing analysis he tried to decrease the error more and more. 
  - He started with a very basic model, of using just the score attribute with a hard threshold for pass or fail grade based on the score value. 
  - After this, to increase accuracy, he analysed the data more found which attributes effect the prediction of the data, and which are not really useful
  - After finding these highly effective attributes, he wrote concrete set of attributs that can be used to assign the grade. Most of them were dependent on 2-3 attributes like Major-Senioriy-Score, Major-Score, or Major-Questions-Score,etc. 
  - Jeremy’s model has achieved accuracy of nearly 87% - beating the perfect prediction model by 4 percentage points. The reasons for these unexpected results were discussed earlier in this section. 


2. *Rohit  Manjunath*    <button class="btn btn-primary" data-toggle="collapse" data-target="#pred12">Rohit's PPT</button>
<div id="pred12" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/rohitpred1.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - Rohit performed well in this prediction challenge, and has a different approach than that of Jeremy's.
  - In Rohit's approach, instead of finding the minimum global threshold of pass or fail based on score, he found the threshold for the maximum score, above which every student passed the class.
  - He then analysed the data based on the Majors first and then found interval threshold for each Majors scores.
  - For some Majors, to increase accuracy, he further explored other attributes in detail to find which effects the final grade.
  - Rohit obtained accuracy of almost 85%, beating the accuracy of perfect model by 2%.
  


---

## Prediction Challenge 2.


Both the training and testing data sets  for this prediction challenge were the same as those in the prediction challenge 1. Students were simply allowed to use the rpart  library  in R to build their prediction models. This provided the opportunity to assess the value of pre-packaged libraries such as rpart as opposed to manual coding the prediction models.  

With rpart() doing most work of prediction in this task, the students were also asked to provide validation for their models prediction power/accuracy. This involved use of cross-validation techniques, which were provided in the form of a single function call,see 6.7.

In general, rpart() offered great help to the vast majority of students. In fact around 70% of all students achieved identical accuracy on the testing data set, which was a few points shy of the ideal 83%. This was the case because  they used the same prediction model - a simple default rpart model. Interestingly rpart()  accuracy did not  beat the few top prediction model solutions for challenge 1. These were coded manually without using any R libraries. In fact nobody has beat the top accuracy achieved for the prediction challenge 1 by Jeremy Prasad!  However he spent a lot of time building his prediction model from the ground. And got away with some overfitting for sure.

To perform this challenge yourself please visit the kaggle site of this prediction challenge. Link to Kaggle Site: Prediction Challenge 2

<!-- To perform this challenge yourself please visit the kaggle site of this prediction challenge. Link to Kaggle Site: [Prediction Challenge 2](https://www.kaggle.com/t/607a8221c6a647048f88ffa380ad1e4b){target="_blank"} -->


<!-- But here we can use the rpart() function of creating the decision tree and predicting on the testing dataset. -->

<!-- - **How the data was generated using R for prediction challenge 2** -->

<!-- ```{r} -->
<!-- library(rpart) -->

<!-- # Load Data -->
<!-- dat<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/M2021test_answer.csv",stringsAsFactors = T) -->

<!-- # Using  rpart() for prediction -->
<!-- tree <- rpart(Grade ~ Attendance+Major+Questions+Score+Seniority+Texting, data=dat,method = "class") -->

<!-- # Predict using the built decision tree. -->
<!-- pred <- predict(tree, newdata = dat, type = "class") -->

<!-- # Confusion matrix for prediction vs actual values. -->
<!-- table(actual = dat[,8], predicted = pred) -->

<!-- # Accuracy of the prediction -->
<!-- mean(pred==dat$Grade) -->

<!-- # Code to display the tree. Cannot be used in this interactive box. -->
<!-- # library(rpart.plot) -->
<!-- # rpart.plot(tree) -->
<!-- ``` -->
<!-- ![Tree Predicted above using the rpart function.](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/predblog/pred2.svg) -->

<!-- - As we can see, the prediction accuracy is near 83%, using just the simple rpart model without any control parameters. -->
<!--   - Students can use the control parameters for creating decision tree with more accuracy. -->
<!-- - **Note**, that the decision tree generated using the rpart() function will be different than that of the ideal trees shown in prediction challenge 1. -->

---

### Top Submissions for Challenge 2

Since rpart() is a very powerful function to find patterns with higher accuracy, the passing criteria for this challenge was above 80% accuracy score.

1. Kevin Larkin    <button class="btn btn-primary" data-toggle="collapse" data-target="#pred21">Kevin's PPT</button>
<div id="pred21" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/kevinpred2.pdf&embedded=true" width="100%" height="500px"></embed>
</div>  
  
  - This was the top submission in terms of accuracy score on Kaggle.
  - Kevin used the rpart() function, for modeling, with all the attributes of the training dataset except *Studentid*.
  - To increase the accuracy of his model, he used the `rpart.control()` function parameters, especially the `cp` parameter of the function, which increased the splitting accuracy.
  - Kevin acheived an accuracy score of over 86% on the test dataset for this challenge.
  


2. Michael Ryvin    <button class="btn btn-primary" data-toggle="collapse" data-target="#pred22">Michael's PPT</button>
<div id="pred22" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/michaelpred2.pdf&embedded=true" width="100%" height="500px"></embed>
</div>
 
  - This was the second best submission as per accuracy score on Kaggle.
  - Michael used the rpart() function, along with some control parameters for creating the decision tree.
  - Michael achieved an accuracy score of over 86% on the test dataset.
 
  
  
3. Shuohao Ping    <button class="btn btn-primary" data-toggle="collapse" data-target="#pred23">Shuohao's PPT</button>
<div id="pred23" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/shuohaopred2.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - This was the third best submission as per accuracy score on Kaggle.
  - Shuohao used multiple iterations to create his final model.
  - In each iteration, Shuohao tried to vary the control parameters and its values to find the best fit model after cross-validation.
  - Shuohao, acheived an accuracy score of over 86% on the test dataset.
  



---

## Prediction Challenge 3.

In  prediction challenge 3, the task was to predict Earnings as a numerical variable, using any ML algorithm.
Earnings variable is part of the Earnings dataset which is a synthetic data set relating earnings of a person to different attributes such as GPA in college, Major, as well as number of personal connections and several other attributes. 

Lets look at a snippet of the Earnings dataset used for training the models below.


```{r,echo=FALSE}
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/Earnings_Train2021.csv") #web load
temp<-knitr::kable(
  head(realestate, 10), caption = 'Snippet of Earnings Dataset(TRAINING) for Prediction Challenge 3',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")

```


<!-- Lets look at the snippet of the earnings dataset for testing. -->

<!-- ```{r,echo=FALSE} -->
<!-- realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/Earnings_Test.csv") #web load -->
<!-- temp<-knitr::kable( -->
<!--   head(realestate, 10), caption = 'Snippet of Earnings Dataset(TESTING) for Prediction Challenge 3', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- library(kableExtra) -->
<!-- kableExtra::scroll_box(temp,width = "100%") -->
<!-- ``` -->

<!-- We can see that the *Earnings* attribute is not present in this dataset, since it is the attribute that will be predicted using our analysis of the training dataset. -->

<!-- Also, lets look at the submission file for prediction challenge 3. -->

<!-- ```{r,echo=FALSE} -->
<!-- realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/earning_submission.csv") #web load -->
<!-- temp<-knitr::kable( -->
<!--   head(realestate, 10), caption = 'Snippet of Submission file for Prediction Challenge 3', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- library(kableExtra) -->
<!-- kableExtra::scroll_box(temp,width = "100%") -->
<!-- ``` -->

<!-- We can see there are only 2 columns *ID* and *Earnings*. *ID* column's entries corresponds/are similar to the *ID* column of the testing data. Thus we need to just fill the *Earnings* column with appropriate earning value predicted by our analysis. -->

<!-- Now that we have seen the data, feel free to go to the Kaggle site of this prediction challenge and take the challenge yourself. The link for challenge: [Prediction Challenge 3](https://www.kaggle.com/t/951a9ad1d7e9444bb29b0dca65aed1cd){target="_blank"} -->


---

### How the data was generated for Challenge 3

The data was generated differently for different values of Education attribute - which described several classes of majors such as STEM, Humanities, Professional etc.

The relationship between earnings and other attributes was generated by different formulas for different values of Education attribute as follows:


``` text

   Stem                 earn = -100 * gpa +10000
   Humanities           earn =  100*  gpa + 10000
   Vocational           earn =  100 * gpa + 13000
   Professional         earn =  -100gpa +12000
   other                earn =  connection ^2 +5000
   business             earn =  gpa  * 100 * parity +10000
                                where parity   = 1 if graduation year = even
                                                 0 if graduation year = odd
```

As we can see, these formulas are mostly linear, while the formula for "other" education attribute is quadratic. Also, for "Business" education attribute subjects, the formula is dependent on an additional attribute. Notice that the rule for business majors was quite tricky - making the earning formula dependent on the student’s graduation year being even or odd!


For more detailed data analysis please view the document attached here. <button class="btn btn-primary" data-toggle="collapse" data-target="#pred3"> Pred 3 Analysis</button>
<div id="pred3" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/sarahpred3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

- **How the data was modeled in R **

```{r,height=800}
# Load the dataset
dat<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/Earnings_Test_answer.csv",stringsAsFactors = T)

# Create a prediction column to store predicted values and append that column to dataset.
dat$predEarnings <- rep(0,nrow(dat))


# Predict Earnings of subjects with Education in STEM field.
dat[dat$Major=='STEM',]$predEarnings <- (-100*dat[dat$Major=='STEM',]$GPA + 10000)

# Predict Earnings of subjects with Education in Humanities field.
dat[dat$Major=='Humanities',]$predEarnings <- (100*dat[dat$Major=='Humanities',]$GPA + 10000)

# Predict Earnings of subjects with Education in Vocational field.
dat[dat$Major=='Vocational',]$predEarnings <- (100*dat[dat$Major=='Vocational',]$GPA + 13000)

# Predict Earnings of subjects with Education in Professional field.
dat[dat$Major=='Professional',]$predEarnings <- (-100*dat[dat$Major=='Professional',]$GPA + 12000)

# Predict Earnings of subjects with Education in Other fields.
dat[dat$Major=='Other',]$predEarnings <- (dat[dat$Major=='Other',]$Number_Of_Professional_Connections^2 + 5000)

# Predict Earnings of subjects with Education in Business field.
dat[dat$Major=='Buisness',]$predEarnings <- (100*dat[dat$Major=='Buisness',]$GPA*((dat[dat$Major=='Buisness',]$Graduation_Year+1)%%2) + 10000)


# Compare the predicted Earnings values with the ideal Earnings values in test data.
library(ModelMetrics)
mse(dat$Earnings,dat$predEarnings)


```

We can see that the perfect model, based on knowledge of the way data was generated, achieved  MSE of around 3300. Certainly prediction models with MSE close to 3300 would be considered very good. Again, at first quite surprisingly, there were quite a number of submissions with MSE of prediction models being far less than 3300.

Explanation of how the perfect model could be so soundly beaten is “getting away” with overfitting models for the case of just single testing data set.
---


### Top Submissions for Challenge 3

For this prediction challenge, the MSE score below 30000 was considered a Passing score.

1. Seok Yim     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred31">Seok's PPT</button>
<div id="pred31" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/seokpred3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - This was the top submission based on MSE score, with a final score less than 100.
  - Seok’s analyzed data using plots first and quickly discovered that subsetting data based on Education is     the way to go
  - For each subset (based on a different value of Education attribute)  he subsequently built a different       model. 
  - In comparison to the perfect model accuracy, this MSE was over one order of magnitude smaller, just like     Nick Whelan’s solution below.  

  
  
2. Nick Whelan     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred32">Nick's PPT</button>
<div id="pred32" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/nickpred3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - This was another top submission based on MSE score, with final score less than 100.
  - The approach to solving the task was different compared to Seok's implementation, but was equally good, with nearly the same prediction power/accuracy.
    - Nick tried to use the randomForest algorithm on the whole dataset as the initial model, but the MSE turned out to be near 25,000.
    - Then he did some free-style analysis and found the linear relationship between various subsets of dataset with the *earnings* value.
    - To implement this he used the fundamentals of linear regression very well while creating a learning model, and also used a quadratic model where needed.
  - This resulted in a very accurate model with low MSE score.


3. Bennett Garcia     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred33">Bennett's PPT</button>
<div id="pred33" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/bennetpred3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>
    
  - Bennett had a final MSE score of below 100 and was one of the top submissions for this challenge.
  - A significantly different learning model was used by Bennett to achieve this low MSE.
    - He first analyzed the data, and found attributes on which the dataset can be subsetted on.
    - Then, he here used Neural Networks as models for prediction on those subsets.
    - This Neural Network approach was very well implemented.
    
---

    
## Prediction Challenge 4.

The challenge 4 involved “hidden” derived attributes. We have hoped that students who combined prior data inspection (plotting) with machine learning packages from R library will be able to achieve very high accuracy, certainly over 90%.  Unfortunately, none of the student solutions came even close to the perfect model accuracy, which was 92%.  Pretty much all student solutions centered around the upper 60s percentile. Even the top 3 solutions barely exceeded 68%! 

Let us first describe the challenge and the way data was generated.

**BOX ON THE BEACH CHALLENGE**

*Mysterious box was found on the beach. *

*Despite spending probably years in the water, it still works! *

*But what does it do? *

*It has four inputs (electric) & a switch. Setting these inputs and different switch positions emits various weird and scary sounds as output in response to the electric signals. *

*It sizzles, gurgles, hisses, ominously tics like a bomb,etc…..but nothing happens - just sounds. So no harm will happen to surroundings.*

*Predict the output sound of the box based on one of the four inputs INPUTs, INPUT1,...INPUT4 (numerical values) and SWITCH which has five discrete positions.* 

Let's look at a snippet of the Box on the Beach dataset used for training the models below. The training describes which sounds have been noted in the laboratory in nearly 20,000 experiments combining different input signals and switch positions.

```{r,echo=FALSE}
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/BlackBoxtrainApril22.csv") #web load
temp<-knitr::kable(
  head(realestate, 10), caption = 'Snippet of Black Box Dataset(TRAINING) for Prediction Challenge 4',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")

```


<!-- Lets look at the snippet of the black box dataset for testing. -->

<!-- ```{r,echo=FALSE} -->
<!-- realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/BlackBoxTestApril22-students.csv") #web load -->
<!-- temp<-knitr::kable( -->
<!--   head(realestate, 10), caption = 'Snippet of Black Box Dataset(TESTING) for Prediction Challenge 4', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- library(kableExtra) -->
<!-- kableExtra::scroll_box(temp,width = "100%") -->
<!-- ``` -->

<!-- We can see that the *Sound* attribute is not present in this dataset, since it is the attribute that will be predicted using our analysis of the training dataset. -->

<!-- Also, lets look at the submission file for prediction challenge 4. -->

<!-- ```{r,echo=FALSE} -->
<!-- realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/BlackBoxTestApril22-submission.csv") #web load -->
<!-- temp<-knitr::kable( -->
<!--   head(realestate, 10), caption = 'Snippet of Submission file for Prediction Challenge 4', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- library(kableExtra) -->
<!-- kableExtra::scroll_box(temp,width = "100%") -->
<!-- ``` -->

<!-- We can see there are only 2 columns *ID* and *Sound*. *ID* column's entries corresponds/are similar to the *ID* column of the testing data. Thus we need to just fill the *Sound* column with appropriate earning value predicted by our analysis. -->

<!-- Now that we have seen the data, feel free to go to the Kaggle site of this prediction challenge and take the challenge yourself. The link for challenge: [Prediction Challenge 4](https://www.kaggle.com/t/423f51ea45be4efea1ddb12fee969cfe){target="_blank"} -->

---

### How the data was generated for Challenge 4

The value of Sound attribute (the output sound of the box) was dependent on the four input attributes:

INPUT1,...INPUT4  and one of the five switch positions: Low, Minimum, Medium, Maximum and High. 
Data was generated in two phases. In the first phase a derived attribute called OUTPUT was defined. In the second phase a simple decision tree using just OUTPUT was created which defined the output sound of the box.

The derived attribute OUTPUT was created as different linear combinations of INPUT1...INPUT4 attributes depending on the value of SWITCH.

First we ordered the values of SWITCH attribute as follows:


``` text

The ordering of Switch position is given as:
  Low = 1
  Minimum = 2
  Medium = 3
  Maximum = 4
  High = 5

if Switch == 1 i.e. "Low" 
    then OUTPUT = Input 1+ 5 * Input 2  - 2 * Input3  + sample(2:5,1)
if Switch == 2  i.e. "Minimum"  
    then OUTPUT = 3* Input 2 - 2 * Input 4  + sample(2:3,1)
else  i.e. Position other than "Low" and "Minimum"
    then OUTPUT =  Input1 ^2 -1.5 * Input 3 + sample(5:10,1)


Then SOUND totally depends on OUTPUT attribute, but is distributed probabilistically over all possible sound.

For example, the SOUND when OUTPUT>150 is distributed as 0, 0, 10, 0, 10, 60, 20.
This number list corresponds to Gargle, Tick, Beep, Kaboom, Rumble, Sizzle, Hiss. And thus we can see that "Sizzle" sound has the max probability of 60%, and is this the most likely sound when the OUTPUT value is above 150.


      OUTPUT > 150 -> Max Probability of finding "Sizzle"
100 < OUTPUT < 150 -> Max Probability of finding "Rumble"
 70 < OUTPUT < 100 -> Max Probability of finding "Kaboom"
 50 < OUTPUT < 70  -> Max Probability of finding "Hiss"
 20 < OUTPUT < 50  -> Max Probability of finding "Tick"
      OUTPUT < 20  -> Max Probability of finding "Gargle"

```

The OUTPUT  variable is created by three different linear combinations of INPUT1,...INPUT4 variables   depending on different values of SWITCH attributes. Notice that the same formula is used for  “Medium”, “Maximum” and “High” positions of the SWITCH. The only two SWITCH positions which have different linear combinations of INPUT1, INPUT2, INPUT3 and INPUT4  defining OUTPUT are “Low” and “Minimum”.

- **How the data was generated for Challenge 4 in R**

```text

# Load The Data
dat<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/BlackBoxTestApril22_answer.csv",stringsAsFactors = T)

dat$OUTPUT <- rep(0,nrow(dat))

dat$OUTPUT <- ((dat$INPUT1^2) - (1.5*dat$INPUT3) + sample(5:10,1))

dat[dat$SWITCH == "Low",]$OUTPUT <- (dat[dat$SWITCH == "Low",]$INPUT1 + (5*dat[dat$SWITCH == "Low",]$INPUT2) - (2*dat[dat$SWITCH == "Low",]$INPUT3) + sample(2:5,1))

dat[dat$SWITCH == "Minimum",]$OUTPUT <- ((3*dat[dat$SWITCH == "Minimum",]$INPUT2) - (2*dat[dat$SWITCH == "Minimum",]$INPUT4) + sample(2:3,1))


dat$predSound <- rep('Empty',nrow(dat))
dat[dat$OUTPUT>150,]$predSound<-'Sizzle'
dat[dat$OUTPUT>=100 & dat$OUTPUT<150,]$predSound<-'Rumble'
dat[dat$OUTPUT>=70 & dat$OUTPUT<100,]$predSound<-'Kaboom'
dat[dat$OUTPUT>=50 & dat$OUTPUT<70,]$predSound<-'Hiss'
dat[dat$OUTPUT>=20 & dat$OUTPUT<50,]$predSound<-'Tick'
dat[dat$OUTPUT<20,]$predSound<-'Gargle'


mean(dat$SOUND==dat$predSound)
```

The accuracy of the perfect model created using the above rules, was 92%.  

To our disappointment this  challenge was not a success. And this is by no means the students fault. It was our fault - since the challenge was really not fair. None of our teaching assistants was able to better the student solutions and therefore each solution had accuracy almost 25% below the accuracy of the perfect model.  In fact almost everyone’s model made the range of 65% to 68%. 

Why such a huge discrepancy? Especially that for the first two challenges, students actually got away with some overfitting and even managed to beat the perfect model by a few percentage points. 

We suspect that adding noise to the linear formulas defining OUTPUT for different SWITCH positions may have created problems for simple rpart() application based just on five independent variables INPUT1...INPUT4, SWITCH.  One certainly could not expect from someone who has no hints about the manner the derived attribute OUTPUT was defined, to be able to find the definition (not to mention even guess the existence of) of OUTPUT. Certainly, if OUTPUT’s definition was known, a simple rpart prediction model  using just OUTPUT would achieve accuracy exceeding 90%.


### Top Submissions for Challenge 4

Since this challenge involved stochastically generated data, the prediction accuracy required for passing this challenge was above 60%.

1. Nicole Coria     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred41">Nicole's PPT</button>
<div id="pred41" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/nicolepred41.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - This was the top submission based on accuracy score, with a final score more than 68.7%
  - The approach to solving this challenge was iterative and trail and error based. 
    - First, since the task is to predict categorical data, she decided to use rpart(directly).
    - Then, over iteration, by varying the control parameters of rpart, she tried to find the model with the highest accuracy.
  - Use of cross-validation also helped in finding the best fit model.
  
  
2. Atharva Patil     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred42">Atharva's PPT</button>
<div id="pred42" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/atharvapred41.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - This was another top submission based on accuracy score, with final score above 68%
  - The approach to solving the task was very well implemented, using external resources too.
    - Atharva tried to analyze the data first. To do this, he used Prof. Imielinski's online platform called [Boundless Analytics](http://www.foreveranalytics.com){target="_blank"}.
      - This online platform has ability to analyze the data automatically, and create plots which only matter or provide more information about the data.
      - It eliminates the need to perform the data analysis manually.
  - Then, he proceeded by building the model using the rpart() function and control parameters.


3. Andrew Scovell     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred43">Andrew's PPT</button>
<div id="pred43" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/andrewpred4.pdf&embedded=true" width="100%" height="500px"></embed>
</div>
    
  - Bennett had a final accuracy score of above 68% and was one of the top submissions for this challenge.
  - He did a very extensive data analysis using all the attributes of the dataset.
    - He also tried analyzing using mean, sums, standard deviation, etc of the numerical inputs.
  - Using the control parameters of the rpart() function he tried to find the best fitting model, and used cross-validation to avoid overfitting.
  
---

To perform any of the above challenges yourself, visit the appropriate links.

1. Prediction Challenge 1 [https://www.kaggle.com/t/8099c3c8bd5940928d102a6ddda0ee3d](https://www.kaggle.com/t/8099c3c8bd5940928d102a6ddda0ee3d){target="_blank"}
1. Prediction Challenge 2 [https://www.kaggle.com/t/607a8221c6a647048f88ffa380ad1e4b](https://www.kaggle.com/t/607a8221c6a647048f88ffa380ad1e4b){target="_blank"}
1. Prediction Challenge 3 [https://www.kaggle.com/t/951a9ad1d7e9444bb29b0dca65aed1cd](https://www.kaggle.com/t/951a9ad1d7e9444bb29b0dca65aed1cd){target="_blank"}
1. Prediction Challenge 4 [https://www.kaggle.com/t/423f51ea45be4efea1ddb12fee969cfe](https://www.kaggle.com/t/423f51ea45be4efea1ddb12fee969cfe){target="_blank"}
    
